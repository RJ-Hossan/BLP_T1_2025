{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:19.534379Z","iopub.execute_input":"2025-10-08T10:55:19.534878Z","iopub.status.idle":"2025-10-08T10:55:24.726510Z","shell.execute_reply.started":"2025-10-08T10:55:19.534852Z","shell.execute_reply":"2025-10-08T10:55:24.725741Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.35.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip show transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:29.113427Z","iopub.execute_input":"2025-10-08T10:55:29.113758Z","iopub.status.idle":"2025-10-08T10:55:31.138497Z","shell.execute_reply.started":"2025-10-08T10:55:29.113734Z","shell.execute_reply":"2025-10-08T10:55:31.137792Z"}},"outputs":[{"name":"stdout","text":"Name: transformers\nVersion: 4.53.3\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.11/dist-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: kaggle-environments, peft, sentence-transformers\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel, XLMRobertaTokenizer, XLMRobertaModel, ElectraTokenizer, ElectraModel\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:33.991959Z","iopub.execute_input":"2025-10-08T10:55:33.992864Z","iopub.status.idle":"2025-10-08T10:55:33.999533Z","shell.execute_reply.started":"2025-10-08T10:55:33.992822Z","shell.execute_reply":"2025-10-08T10:55:33.998669Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load datasets\ndf_train = pd.read_csv('/kaggle/input/blp-2025-task-1/data/subtask_1C/blp25_hatespeech_subtask_1C_train.tsv', sep=\"\\t\")\ndf_valid = pd.read_csv('/kaggle/input/blp-2025-task-1/data/subtask_1C/blp25_hatespeech_subtask_1C_dev.tsv', sep=\"\\t\")\ndf_test = pd.read_csv('/kaggle/input/blp-2025-task-1/data/subtask_1C/blp25_hatespeech_subtask_1C_test.tsv', sep=\"\\t\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:36.763241Z","iopub.execute_input":"2025-10-08T10:55:36.763956Z","iopub.status.idle":"2025-10-08T10:55:37.211318Z","shell.execute_reply.started":"2025-10-08T10:55:36.763932Z","shell.execute_reply":"2025-10-08T10:55:37.210651Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df_train.head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:39.739412Z","iopub.execute_input":"2025-10-08T10:55:39.739708Z","iopub.status.idle":"2025-10-08T10:55:39.764386Z","shell.execute_reply.started":"2025-10-08T10:55:39.739688Z","shell.execute_reply":"2025-10-08T10:55:39.763772Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id                                               text hate_type  \\\n0   147963  ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...       NaN   \n1   214275  ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...       NaN   \n2   849172          অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে   Abusive   \n3   821985  চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...       NaN   \n4   477288  এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...   Abusive   \n5   933728    তুরা কিসের জন্য দুভাই যাবি অন্য দেশে কেন জাস না       NaN   \n6   398351  দেশ বিভাগের সময়ে পশ্চিম পাকিস্তানে ২৫ শতাংশ স...       NaN   \n7   786609  ইরান পারমাণবিক বোমা বানাবে বানাবে বলতে বলতে বি...   Abusive   \n8   917115                                  আজকে এই উৎসব কেনো       NaN   \n9   415453  ইমরান ছাড়া পাকিস্তান কখনোই ঘুরে দাড়াতে পারবে ন...       NaN   \n10  243207  গরিবের রক্ত চুসে খাচ্ছে আপনার ব্যবসায়ীরা যেভা...       NaN   \n11  124917  মুসলিম বাচ্চাগুলো বাচ্চা পেরে পেরে গোটা পৃথিবী...   Profane   \n12  450066                      এইটা একমাত্র বাংলাদেশেই সম্ভব       NaN   \n13  615093                      বাংলা সবার মা জননী শেখ হাসিনা       NaN   \n14   56034                         মাহী আপু মাঠ গরম করে ফেলছে       NaN   \n\n     hate_severity     to_whom  \n0   Little to None         NaN  \n1   Little to None         NaN  \n2   Little to None  Individual  \n3   Little to None         NaN  \n4           Severe  Individual  \n5   Little to None         NaN  \n6   Little to None         NaN  \n7   Little to None     Society  \n8   Little to None         NaN  \n9   Little to None         NaN  \n10  Little to None         NaN  \n11          Severe   Community  \n12  Little to None         NaN  \n13  Little to None         NaN  \n14  Little to None         NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>hate_type</th>\n      <th>hate_severity</th>\n      <th>to_whom</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>147963</td>\n      <td>ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>214275</td>\n      <td>ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>849172</td>\n      <td>অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে</td>\n      <td>Abusive</td>\n      <td>Little to None</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>821985</td>\n      <td>চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>477288</td>\n      <td>এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...</td>\n      <td>Abusive</td>\n      <td>Severe</td>\n      <td>Individual</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>933728</td>\n      <td>তুরা কিসের জন্য দুভাই যাবি অন্য দেশে কেন জাস না</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>398351</td>\n      <td>দেশ বিভাগের সময়ে পশ্চিম পাকিস্তানে ২৫ শতাংশ স...</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>786609</td>\n      <td>ইরান পারমাণবিক বোমা বানাবে বানাবে বলতে বলতে বি...</td>\n      <td>Abusive</td>\n      <td>Little to None</td>\n      <td>Society</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>917115</td>\n      <td>আজকে এই উৎসব কেনো</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>415453</td>\n      <td>ইমরান ছাড়া পাকিস্তান কখনোই ঘুরে দাড়াতে পারবে ন...</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>243207</td>\n      <td>গরিবের রক্ত চুসে খাচ্ছে আপনার ব্যবসায়ীরা যেভা...</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>124917</td>\n      <td>মুসলিম বাচ্চাগুলো বাচ্চা পেরে পেরে গোটা পৃথিবী...</td>\n      <td>Profane</td>\n      <td>Severe</td>\n      <td>Community</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>450066</td>\n      <td>এইটা একমাত্র বাংলাদেশেই সম্ভব</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>615093</td>\n      <td>বাংলা সবার মা জননী শেখ হাসিনা</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56034</td>\n      <td>মাহী আপু মাঠ গরম করে ফেলছে</td>\n      <td>NaN</td>\n      <td>Little to None</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:42.776686Z","iopub.execute_input":"2025-10-08T10:55:42.777165Z","iopub.status.idle":"2025-10-08T10:55:42.796056Z","shell.execute_reply.started":"2025-10-08T10:55:42.777143Z","shell.execute_reply":"2025-10-08T10:55:42.795510Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                  id\ncount   35522.000000\nmean   470131.934435\nstd    271256.805054\nmin        96.000000\n25%    235179.000000\n50%    470483.000000\n75%    705906.250000\nmax    939762.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>35522.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>470131.934435</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>271256.805054</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>96.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>235179.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>470483.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>705906.250000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>939762.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:45.989385Z","iopub.execute_input":"2025-10-08T10:55:45.989713Z","iopub.status.idle":"2025-10-08T10:55:46.019468Z","shell.execute_reply.started":"2025-10-08T10:55:45.989691Z","shell.execute_reply":"2025-10-08T10:55:46.018664Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 35522 entries, 0 to 35521\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   id             35522 non-null  int64 \n 1   text           35522 non-null  object\n 2   hate_type      15568 non-null  object\n 3   hate_severity  35522 non-null  object\n 4   to_whom        14332 non-null  object\ndtypes: int64(1), object(4)\nmemory usage: 1.4+ MB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(df_valid.head(10), df_valid.info(), df_valid.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:48.751650Z","iopub.execute_input":"2025-10-08T10:55:48.751931Z","iopub.status.idle":"2025-10-08T10:55:48.767189Z","shell.execute_reply.started":"2025-10-08T10:55:48.751911Z","shell.execute_reply":"2025-10-08T10:55:48.766496Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2512 entries, 0 to 2511\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   id             2512 non-null   int64 \n 1   text           2512 non-null   object\n 2   hate_type      1061 non-null   object\n 3   hate_severity  2512 non-null   object\n 4   to_whom        976 non-null    object\ndtypes: int64(1), object(4)\nmemory usage: 98.3+ KB\n       id                                               text       hate_type  \\\n0  166449  ইন্ডিয়া কি মাছ ধরা বন্ধ রাখছেএক নদীতে দুইনীতি ...  Political Hate   \n1  267692  লক্ষ টাকা ঘুষ দিয়ে অযোগ্য আর দায়িত্বহীন মানস...         Abusive   \n2  184031                                    ওহা ভবনের দালাল             NaN   \n3  939131  আর কতো শিখবে আমার সোনার ছেলেরা এগুলো কে টাকা দ...         Abusive   \n4  210284                             কি সাংঘাতিক ভাই রে তুই         Abusive   \n5  712332        লঞ্চ মালিকদের অভিশপ্ত চক্ষু পদ্মা সেতুর উপর         Abusive   \n6  588722  লুটপাট করে বিদেশে বাড়ি কিনেন যেভাবে খুশি খরচ ক...         Abusive   \n7  502295                              ওই কুকুরের শাস্তি চাই         Abusive   \n8  818284  সাবাস রাশিয়া অনেক অভিনন্দন রহিলজয় নিশ্চয় তোমাদ...             NaN   \n9  287015  শালার ব্যাটা খুব বকা দিতে পারে ইচ্ছা করে না নি...         Profane   \n\n    hate_severity       to_whom  \n0            Mild       Society  \n1  Little to None  Organization  \n2  Little to None           NaN  \n3            Mild    Individual  \n4  Little to None    Individual  \n5            Mild     Community  \n6            Mild    Individual  \n7  Little to None    Individual  \n8  Little to None           NaN  \n9          Severe    Individual   None                   id\ncount    2512.000000\nmean   480864.673567\nstd    272206.796753\nmin       567.000000\n25%    244570.250000\n50%    487492.500000\n75%    715322.000000\nmax    939569.000000\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Preprocess: replace NaN with 'None'\nfor df in [df_train, df_valid]:\n    df['hate_type'] = df['hate_type'].fillna('None')\n    df['hate_severity'] = df['hate_severity'].fillna('None')\n    df['to_whom'] = df['to_whom'].fillna('None')\n\n# Get unique labels\nhate_type_unique = ['None', 'Abusive', 'Profane', 'Religious Hate', 'Political Hate', 'Sexism']\nhate_severity_unique = ['Little to None', 'Severe', 'Mild', 'None']\nto_whom_unique = ['None', 'Individual', 'Society', 'Community', 'Organization']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:54.203580Z","iopub.execute_input":"2025-10-08T10:55:54.204069Z","iopub.status.idle":"2025-10-08T10:55:54.218114Z","shell.execute_reply.started":"2025-10-08T10:55:54.204047Z","shell.execute_reply":"2025-10-08T10:55:54.217549Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Label Encoders\nle_hate_type = LabelEncoder().fit(hate_type_unique)\nle_hate_severity = LabelEncoder().fit(hate_severity_unique)\nle_to_whom = LabelEncoder().fit(to_whom_unique)\n\nnum_hate_types = len(hate_type_unique)\nnum_severities = len(hate_severity_unique)\nnum_to_whoms = len(to_whom_unique)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:55:56.834208Z","iopub.execute_input":"2025-10-08T10:55:56.834487Z","iopub.status.idle":"2025-10-08T10:55:56.839253Z","shell.execute_reply.started":"2025-10-08T10:55:56.834466Z","shell.execute_reply":"2025-10-08T10:55:56.838575Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(le_hate_type, le_hate_severity, le_to_whom)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:01.934151Z","iopub.execute_input":"2025-10-08T10:56:01.934757Z","iopub.status.idle":"2025-10-08T10:56:01.953798Z","shell.execute_reply.started":"2025-10-08T10:56:01.934735Z","shell.execute_reply":"2025-10-08T10:56:01.953193Z"}},"outputs":[{"name":"stdout","text":"LabelEncoder() LabelEncoder() LabelEncoder()\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(num_hate_types, num_severities, num_to_whoms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:03.834548Z","iopub.execute_input":"2025-10-08T10:56:03.835192Z","iopub.status.idle":"2025-10-08T10:56:03.839118Z","shell.execute_reply.started":"2025-10-08T10:56:03.835169Z","shell.execute_reply":"2025-10-08T10:56:03.838358Z"}},"outputs":[{"name":"stdout","text":"6 4 5\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Encode labels for train and valid\ndf_train['hate_type_label'] = le_hate_type.transform(df_train['hate_type'])\ndf_train['hate_severity_label'] = le_hate_severity.transform(df_train['hate_severity'])\ndf_train['to_whom_label'] = le_to_whom.transform(df_train['to_whom'])\n\ndf_valid['hate_type_label'] = le_hate_type.transform(df_valid['hate_type'])\ndf_valid['hate_severity_label'] = le_hate_severity.transform(df_valid['hate_severity'])\ndf_valid['to_whom_label'] = le_to_whom.transform(df_valid['to_whom'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:05.929082Z","iopub.execute_input":"2025-10-08T10:56:05.929565Z","iopub.status.idle":"2025-10-08T10:56:05.988478Z","shell.execute_reply.started":"2025-10-08T10:56:05.929540Z","shell.execute_reply":"2025-10-08T10:56:05.987755Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"df_train.head(15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:10.391977Z","iopub.execute_input":"2025-10-08T10:56:10.392615Z","iopub.status.idle":"2025-10-08T10:56:10.402568Z","shell.execute_reply.started":"2025-10-08T10:56:10.392591Z","shell.execute_reply":"2025-10-08T10:56:10.401674Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"        id                                               text hate_type  \\\n0   147963  ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...      None   \n1   214275  ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...      None   \n2   849172          অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে   Abusive   \n3   821985  চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...      None   \n4   477288  এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...   Abusive   \n5   933728    তুরা কিসের জন্য দুভাই যাবি অন্য দেশে কেন জাস না      None   \n6   398351  দেশ বিভাগের সময়ে পশ্চিম পাকিস্তানে ২৫ শতাংশ স...      None   \n7   786609  ইরান পারমাণবিক বোমা বানাবে বানাবে বলতে বলতে বি...   Abusive   \n8   917115                                  আজকে এই উৎসব কেনো      None   \n9   415453  ইমরান ছাড়া পাকিস্তান কখনোই ঘুরে দাড়াতে পারবে ন...      None   \n10  243207  গরিবের রক্ত চুসে খাচ্ছে আপনার ব্যবসায়ীরা যেভা...      None   \n11  124917  মুসলিম বাচ্চাগুলো বাচ্চা পেরে পেরে গোটা পৃথিবী...   Profane   \n12  450066                      এইটা একমাত্র বাংলাদেশেই সম্ভব      None   \n13  615093                      বাংলা সবার মা জননী শেখ হাসিনা      None   \n14   56034                         মাহী আপু মাঠ গরম করে ফেলছে      None   \n\n     hate_severity     to_whom  hate_type_label  hate_severity_label  \\\n0   Little to None        None                1                    0   \n1   Little to None        None                1                    0   \n2   Little to None  Individual                0                    0   \n3   Little to None        None                1                    0   \n4           Severe  Individual                0                    3   \n5   Little to None        None                1                    0   \n6   Little to None        None                1                    0   \n7   Little to None     Society                0                    0   \n8   Little to None        None                1                    0   \n9   Little to None        None                1                    0   \n10  Little to None        None                1                    0   \n11          Severe   Community                3                    3   \n12  Little to None        None                1                    0   \n13  Little to None        None                1                    0   \n14  Little to None        None                1                    0   \n\n    to_whom_label  \n0               2  \n1               2  \n2               1  \n3               2  \n4               1  \n5               2  \n6               2  \n7               4  \n8               2  \n9               2  \n10              2  \n11              0  \n12              2  \n13              2  \n14              2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>hate_type</th>\n      <th>hate_severity</th>\n      <th>to_whom</th>\n      <th>hate_type_label</th>\n      <th>hate_severity_label</th>\n      <th>to_whom_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>147963</td>\n      <td>ধন্যবাদ বর্ডার গার্ড দেরকে এভাবে পাহারা দিতে হ...</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>214275</td>\n      <td>ছোটবেলায় অনেক কষ্ট করে কিছু গালাগালি শিখছিলাম...</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>849172</td>\n      <td>অতিরিক্ত এ নিজেকে বাদুর বানাইয়া ফেলছেন রে</td>\n      <td>Abusive</td>\n      <td>Little to None</td>\n      <td>Individual</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>821985</td>\n      <td>চিন ভারত রাশিয়া এই তিন দেশ এক থাকলে বিশ্বকে শা...</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>477288</td>\n      <td>এটার বিচার কে করবেযে বিচার করবে সেই তো হলো এই ...</td>\n      <td>Abusive</td>\n      <td>Severe</td>\n      <td>Individual</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>933728</td>\n      <td>তুরা কিসের জন্য দুভাই যাবি অন্য দেশে কেন জাস না</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>398351</td>\n      <td>দেশ বিভাগের সময়ে পশ্চিম পাকিস্তানে ২৫ শতাংশ স...</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>786609</td>\n      <td>ইরান পারমাণবিক বোমা বানাবে বানাবে বলতে বলতে বি...</td>\n      <td>Abusive</td>\n      <td>Little to None</td>\n      <td>Society</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>917115</td>\n      <td>আজকে এই উৎসব কেনো</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>415453</td>\n      <td>ইমরান ছাড়া পাকিস্তান কখনোই ঘুরে দাড়াতে পারবে ন...</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>243207</td>\n      <td>গরিবের রক্ত চুসে খাচ্ছে আপনার ব্যবসায়ীরা যেভা...</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>124917</td>\n      <td>মুসলিম বাচ্চাগুলো বাচ্চা পেরে পেরে গোটা পৃথিবী...</td>\n      <td>Profane</td>\n      <td>Severe</td>\n      <td>Community</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>450066</td>\n      <td>এইটা একমাত্র বাংলাদেশেই সম্ভব</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>615093</td>\n      <td>বাংলা সবার মা জননী শেখ হাসিনা</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>56034</td>\n      <td>মাহী আপু মাঠ গরম করে ফেলছে</td>\n      <td>None</td>\n      <td>Little to None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Dataset Class\nclass MultiTaskDataset(Dataset):\n    def __init__(self, texts, hate_types=None, severities=None, to_whoms=None, tokenizer=None, max_len=128):\n        self.texts = texts\n        self.hate_types = hate_types\n        self.severities = severities\n        self.to_whoms = to_whoms\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        item = {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n        }\n\n        if self.hate_types is not None:\n            item['hate_type'] = torch.tensor(self.hate_types[idx], dtype=torch.long)\n            item['hate_severity'] = torch.tensor(self.severities[idx], dtype=torch.long)\n            item['to_whom'] = torch.tensor(self.to_whoms[idx], dtype=torch.long)\n\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:13.403633Z","iopub.execute_input":"2025-10-08T10:56:13.403883Z","iopub.status.idle":"2025-10-08T10:56:13.410664Z","shell.execute_reply.started":"2025-10-08T10:56:13.403864Z","shell.execute_reply":"2025-10-08T10:56:13.409831Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Multi-task model base\nclass MultiTaskModel(nn.Module):\n    def __init__(self, base_model, num_hate_types, num_severities, num_to_whoms):\n        super().__init__()\n        self.base_model = base_model\n        self.dropout = nn.Dropout(0.1)\n        self.hate_type_head = nn.Linear(self.base_model.config.hidden_size, num_hate_types)\n        self.severity_head = nn.Linear(self.base_model.config.hidden_size, num_severities)\n        self.to_whom_head = nn.Linear(self.base_model.config.hidden_size, num_to_whoms)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs.last_hidden_state[:, 0]\n        dropout_output = self.dropout(pooled_output)\n        hate_type_logits = self.hate_type_head(dropout_output)\n        severity_logits = self.severity_head(dropout_output)\n        to_whom_logits = self.to_whom_head(dropout_output)\n        return hate_type_logits, severity_logits, to_whom_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:17.437992Z","iopub.execute_input":"2025-10-08T10:56:17.438475Z","iopub.status.idle":"2025-10-08T10:56:17.447038Z","shell.execute_reply.started":"2025-10-08T10:56:17.438442Z","shell.execute_reply":"2025-10-08T10:56:17.446183Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Training function\ndef train_model(model, train_loader, valid_loader, optimizer, scheduler, device, epochs=2):\n    model = nn.DataParallel(model).to(device)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            hate_type = batch['hate_type'].to(device)\n            hate_severity = batch['hate_severity'].to(device)\n            to_whom = batch['to_whom'].to(device)\n\n            optimizer.zero_grad()\n            hate_type_logits, severity_logits, to_whom_logits = model(input_ids, attention_mask)\n\n            loss = (loss_fn(hate_type_logits, hate_type) +\n                    loss_fn(severity_logits, hate_severity) +\n                    loss_fn(to_whom_logits, to_whom))\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}')\n\n        # Validation\n        model.eval()\n        val_hate_type_preds, val_hate_type_true = [], []\n        val_severity_preds, val_severity_true = [], []\n        val_to_whom_preds, val_to_whom_true = [], []\n        with torch.no_grad():\n            for batch in valid_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                hate_type = batch['hate_type'].to(device)\n                hate_severity = batch['hate_severity'].to(device)\n                to_whom = batch['to_whom'].to(device)\n\n                hate_type_logits, severity_logits, to_whom_logits = model(input_ids, attention_mask)\n\n                val_hate_type_preds.extend(torch.argmax(hate_type_logits, dim=1).cpu().numpy())\n                val_hate_type_true.extend(hate_type.cpu().numpy())\n                val_severity_preds.extend(torch.argmax(severity_logits, dim=1).cpu().numpy())\n                val_severity_true.extend(hate_severity.cpu().numpy())\n                val_to_whom_preds.extend(torch.argmax(to_whom_logits, dim=1).cpu().numpy())\n                val_to_whom_true.extend(to_whom.cpu().numpy())\n\n        hate_type_acc = accuracy_score(val_hate_type_true, val_hate_type_preds)\n        severity_acc = accuracy_score(val_severity_true, val_severity_preds)\n        to_whom_acc = accuracy_score(val_to_whom_true, val_to_whom_preds)\n        print(f'Validation - Hate Type Acc: {hate_type_acc:.4f}, Severity Acc: {severity_acc:.4f}, To Whom Acc: {to_whom_acc:.4f}')\n\n    return model.module  # Return the original model without DataParallel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:22.043497Z","iopub.execute_input":"2025-10-08T10:56:22.043753Z","iopub.status.idle":"2025-10-08T10:56:22.054674Z","shell.execute_reply.started":"2025-10-08T10:56:22.043734Z","shell.execute_reply":"2025-10-08T10:56:22.053862Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Adjusted prediction function to handle different tokenizers\ndef predict_ensemble_separate(models, test_datasets, device):\n    for model in models:\n        model = nn.DataParallel(model).to(device)\n        model.eval()\n\n    # Assume test_datasets = [test_dataset_bangla, test_dataset_xlmr, ...]\n    test_loaders = [DataLoader(ds, batch_size=32) for ds in test_datasets]\n\n    num_samples = len(test_datasets[0])\n    hate_type_logits_sum = torch.zeros((num_samples, num_hate_types), device=device)\n    severity_logits_sum = torch.zeros((num_samples, num_severities), device=device)\n    to_whom_logits_sum = torch.zeros((num_samples, num_to_whoms), device=device)\n\n    with torch.no_grad():\n        for loader, model in zip(test_loaders, models):\n            local_idx = 0\n            for batch in loader:\n                batch_size = batch['input_ids'].size(0)\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n\n                hate_type_logits, severity_logits, to_whom_logits = model(input_ids, attention_mask)\n\n                hate_type_logits_sum[local_idx:local_idx+batch_size] += hate_type_logits\n                severity_logits_sum[local_idx:local_idx+batch_size] += severity_logits\n                to_whom_logits_sum[local_idx:local_idx+batch_size] += to_whom_logits\n\n                local_idx += batch_size\n\n    # Average\n    num_models = len(models)\n    hate_type_avg_logits = hate_type_logits_sum / num_models\n    severity_avg_logits = severity_logits_sum / num_models\n    to_whom_avg_logits = to_whom_logits_sum / num_models\n\n    hate_type_probs = torch.softmax(hate_type_avg_logits, dim=1).cpu().numpy()\n    severity_probs = torch.softmax(severity_avg_logits, dim=1).cpu().numpy()\n    to_whom_probs = torch.softmax(to_whom_avg_logits, dim=1).cpu().numpy()\n\n    hate_type_pred = np.argmax(hate_type_probs, axis=1)\n    severity_pred = np.argmax(severity_probs, axis=1)\n    to_whom_pred = np.argmax(to_whom_probs, axis=1)\n\n    hate_type_labels = le_hate_type.inverse_transform(hate_type_pred)\n    severity_labels = le_hate_severity.inverse_transform(severity_pred)\n    to_whom_labels = le_to_whom.inverse_transform(to_whom_pred)\n\n    return hate_type_labels, severity_labels, to_whom_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:27.549735Z","iopub.execute_input":"2025-10-08T10:56:27.549992Z","iopub.status.idle":"2025-10-08T10:56:27.558164Z","shell.execute_reply.started":"2025-10-08T10:56:27.549975Z","shell.execute_reply":"2025-10-08T10:56:27.557288Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Main\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:56:30.075758Z","iopub.execute_input":"2025-10-08T10:56:30.076021Z","iopub.status.idle":"2025-10-08T10:56:30.081317Z","shell.execute_reply.started":"2025-10-08T10:56:30.076000Z","shell.execute_reply":"2025-10-08T10:56:30.080622Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Bangla-BERT (sagorsarker/bangla-bert-base)\nbangla_tokenizer = BertTokenizer.from_pretrained('sagorsarker/bangla-bert-base')\nbangla_base = BertModel.from_pretrained('sagorsarker/bangla-bert-base')\nmodel_bangla = MultiTaskModel(bangla_base, num_hate_types, num_severities, num_to_whoms)\n\ntrain_dataset_bangla = MultiTaskDataset(df_train['text'].tolist(), df_train['hate_type_label'].tolist(), df_train['hate_severity_label'].tolist(), df_train['to_whom_label'].tolist(), bangla_tokenizer)\nvalid_dataset_bangla = MultiTaskDataset(df_valid['text'].tolist(), df_valid['hate_type_label'].tolist(), df_valid['hate_severity_label'].tolist(), df_valid['to_whom_label'].tolist(), bangla_tokenizer)\ntest_dataset_bangla = MultiTaskDataset(df_test['text'].tolist(), tokenizer=bangla_tokenizer)\n\ntrain_loader_bangla = DataLoader(train_dataset_bangla, batch_size=32, shuffle=True)\nvalid_loader_bangla = DataLoader(valid_dataset_bangla, batch_size=32)\n\noptimizer_bangla = AdamW(model_bangla.parameters(), lr=2.5e-5)\ntotal_steps = len(train_loader_bangla) * 2\nscheduler_bangla = get_linear_schedule_with_warmup(optimizer_bangla, num_warmup_steps=0, num_training_steps=total_steps)\n\nmodel_bangla = train_model(model_bangla, train_loader_bangla, valid_loader_bangla, optimizer_bangla, scheduler_bangla, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:47:17.062531Z","iopub.execute_input":"2025-10-08T10:47:17.062883Z","iopub.status.idle":"2025-10-08T10:47:17.067303Z","shell.execute_reply.started":"2025-10-08T10:47:17.062843Z","shell.execute_reply":"2025-10-08T10:47:17.066471Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# XLMR (xlm-roberta-base)\nxlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\nxlmr_base = XLMRobertaModel.from_pretrained('xlm-roberta-base')\nmodel_xlmr = MultiTaskModel(xlmr_base, num_hate_types, num_severities, num_to_whoms)\n\ntrain_dataset_xlmr = MultiTaskDataset(df_train['text'].tolist(), df_train['hate_type_label'].tolist(), df_train['hate_severity_label'].tolist(), df_train['to_whom_label'].tolist(), xlmr_tokenizer)\nvalid_dataset_xlmr = MultiTaskDataset(df_valid['text'].tolist(), df_valid['hate_type_label'].tolist(), df_valid['hate_severity_label'].tolist(), df_valid['to_whom_label'].tolist(), xlmr_tokenizer)\ntest_dataset_xlmr = MultiTaskDataset(df_test['text'].tolist(), tokenizer=xlmr_tokenizer)\n\ntrain_loader_xlmr = DataLoader(train_dataset_xlmr, batch_size=32, shuffle=True)\nvalid_loader_xlmr = DataLoader(valid_dataset_xlmr, batch_size=32)\n\noptimizer_xlmr = AdamW(model_xlmr.parameters(), lr=2e-5)\ntotal_steps = len(train_loader_xlmr) * 2\nscheduler_xlmr = get_linear_schedule_with_warmup(optimizer_xlmr, num_warmup_steps=0, num_training_steps=total_steps)\n\nmodel_xlmr = train_model(model_xlmr, train_loader_xlmr, valid_loader_xlmr, optimizer_xlmr, scheduler_xlmr, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:36:15.109782Z","iopub.execute_input":"2025-10-08T10:36:15.110030Z","iopub.status.idle":"2025-10-08T10:36:15.114200Z","shell.execute_reply.started":"2025-10-08T10:36:15.110013Z","shell.execute_reply":"2025-10-08T10:36:15.113530Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# BUET BanglaBERT (csebuetnlp/banglabert)\nbuet_tokenizer = ElectraTokenizer.from_pretrained('csebuetnlp/banglabert')\nbuet_base = ElectraModel.from_pretrained('csebuetnlp/banglabert')\nmodel_buet = MultiTaskModel(buet_base, num_hate_types, num_severities, num_to_whoms)\n\ntrain_dataset_buet = MultiTaskDataset(df_train['text'].tolist(), df_train['hate_type_label'].tolist(), df_train['hate_severity_label'].tolist(), df_train['to_whom_label'].tolist(), buet_tokenizer)\nvalid_dataset_buet = MultiTaskDataset(df_valid['text'].tolist(), df_valid['hate_type_label'].tolist(), df_valid['hate_severity_label'].tolist(), df_valid['to_whom_label'].tolist(), buet_tokenizer)\ntest_dataset_buet = MultiTaskDataset(df_test['text'].tolist(), tokenizer=buet_tokenizer)\n\ntrain_loader_buet = DataLoader(train_dataset_buet, batch_size=32, shuffle=True)\nvalid_loader_buet = DataLoader(valid_dataset_buet, batch_size=32)\n\noptimizer_buet = AdamW(model_buet.parameters(), lr=2e-5)\ntotal_steps = len(train_loader_buet) * 2\nscheduler_buet = get_linear_schedule_with_warmup(optimizer_buet, num_warmup_steps=0, num_training_steps=total_steps)\n\nmodel_buet = train_model(model_buet, train_loader_buet, valid_loader_buet, optimizer_buet, scheduler_buet, device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-08T10:37:11.907527Z","iopub.execute_input":"2025-10-08T10:37:11.908064Z","iopub.status.idle":"2025-10-08T10:37:11.911524Z","shell.execute_reply.started":"2025-10-08T10:37:11.908039Z","shell.execute_reply":"2025-10-08T10:37:11.910841Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Test datasets\ntest_datasets = [test_dataset_bangla, test_dataset_xlmr, test_dataset_buet]\nmodels = [model_bangla, model_xlmr, model_buet]\n\nhate_type_pred, severity_pred, to_whom_pred = predict_ensemble_separate(models, test_datasets, device)\n\n# Add to df_test\ndf_test['hate_type'] = hate_type_pred\ndf_test['hate_severity'] = severity_pred\ndf_test['to_whom'] = to_whom_pred\ndf_test['model'] = 'ensemble'\n\n# Save predictions\ndf_test[['id', 'hate_type', 'hate_severity', 'to_whom', 'model']].to_csv('subtask_1C.tsv', sep='\\t', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}